{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2842dde",
   "metadata": {},
   "source": [
    "Experiment 1\n",
    "Objectives : \n",
    "1. Combine DinoV3-ViTS+ and Gemma-3-270M and get it to output something\n",
    "2. build the data pipeline for MS-COCO image captioning dataset. test the model on the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "gemma_id = \"google/gemma-3-270m\"\n",
    "vit_id = \"facebook/dinov3-vits16plus-pretrain-lvd1689m\"\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_id)\n",
    "vit_tokenizer = AutoImageProcessor.from_pretrained(vit_id)\n",
    "\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_id,\n",
    "    dtype=torch.float32,\n",
    "    device_map = \"auto\",\n",
    ").to(device)\n",
    "\n",
    "vit = AutoModel.from_pretrained(\n",
    "    vit_id,\n",
    "    dtype=torch.float32,\n",
    "    device_map = \"auto\",\n",
    ").to(device)\n",
    "\n",
    "for parameter in vit.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_embed_size =  gemma.get_input_embeddings().weight.shape[1]\n",
    "vit_embed_size = vit.config.hidden_size\n",
    "\n",
    "mlp_adapter = nn.Sequential(\n",
    "    nn.LayerNorm(vit_embed_size),\n",
    "    nn.Linear(vit_embed_size, gemma_embed_size),\n",
    "    nn.GELU(approximate='tanh'),\n",
    "    nn.Linear(gemma_embed_size, gemma_embed_size)\n",
    ")\n",
    "\n",
    "def prepare_inputs(images, captions):\n",
    "    with torch.no_grad():\n",
    "        vit_out = vit(images)\n",
    "\n",
    "    image_embed = mlp_adapter(vit_out)\n",
    "\n",
    "    tok = gemma_tokenizer(captions, return_tensors='pt', padding=True)\n",
    "    input_ids = tok[\"input_ids\"].to(device)\n",
    "    attention_mask = tok[\"attention_mask\"].to(device)\n",
    "\n",
    "    input_embeds = gemma.get_input_embeddings()(input_ids)\n",
    "    bs, num_txt_tok, gem_dim = input_embeds.shape\n",
    "    num_img_tok = image_embed.shape[1]\n",
    "\n",
    "    new_embeds = torch.cat([image_embed, input_embeds], dim=1) # (bs, img_tok+txt_tok, gem_dim)\n",
    "    new_mask = torch.cat([torch.ones(bs, num_img_tok, device=device), attention_mask], dim=1) #(bs, img_tok+txt_tok, gem_dim)\n",
    "\n",
    "    return new_embeds, new_mask, input_ids, num_img_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"lmms-lab/COCO-Caption2017\", split=\"train\")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209479d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
