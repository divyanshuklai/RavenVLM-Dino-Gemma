{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87556180",
   "metadata": {},
   "source": [
    "# Captioner Diagnostics Notebook\n",
    "\n",
    "Purpose: step-by-step checks to diagnose NaN losses, validation length, data consistency, and mixed precision issues for Gemma+DINO caption model.\n",
    "\n",
    "You can run cells independently. If a cell raises/asserts, fix the noted issue then re-run.\n",
    "\n",
    "Sections:\n",
    "1. Environment & Versions\n",
    "2. Project Path + Repro Settings\n",
    "3. Dataloaders (train/val) Build\n",
    "4. Inspect First Batch (shapes, channels, sample caption)\n",
    "5. Channel / Grayscale Scan (optional fix)\n",
    "6. Build Model + Param Count\n",
    "7. Parameter NaN / Inf Scan\n",
    "8. Single Forward (FP32) Without Lightning\n",
    "9. Tokenization Sanity (empty / all-pad detection)\n",
    "10. Autocast (FP16) Forward Test\n",
    "11. Gradient Step (detect exploding grad / NaN)\n",
    "12. Mini Lightning Trainer (few steps, limited val)\n",
    "13. Full Dataset Channel Distribution Summary\n",
    "14. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b66fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python        : 3.13.3\n",
      "Platform      : Linux-6.14.0-29-generic-x86_64-with-glibc2.39\n",
      "Torch         : 2.8.0+cu128\n",
      "Transformers  : 4.56.1\n",
      "Datasets      : 4.1.0\n",
      "Lightning     : 2.5.5\n",
      "CUDA available: True\n",
      "GPU           : NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# 1. Environment & Versions\n",
    "import os, sys, platform, math, json, random\n",
    "import torch, transformers, datasets as hfds, pytorch_lightning as pl\n",
    "print('Python        :', sys.version.split()[0])\n",
    "print('Platform      :', platform.platform())\n",
    "print('Torch         :', torch.__version__)\n",
    "print('Transformers  :', transformers.__version__)\n",
    "print('Datasets      :', hfds.__version__)\n",
    "print('Lightning     :', pl.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU           :', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a835c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/divyansh/Documents/SEM4/DL/Project\n",
      "HF cache: None\n"
     ]
    }
   ],
   "source": [
    "# 2. Project Path + Repro Settings\n",
    "import os, sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..')) if os.path.basename(os.getcwd()) == 'tests' else os.path.abspath(os.path.join(os.getcwd()))\n",
    "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "os.environ.setdefault('TOKENIZERS_PARALLELISM','false')\n",
    "os.environ.setdefault('WANDB_MODE','disabled')\n",
    "HF_CACHE = os.environ.get('HF_HOME') or None\n",
    "print('HF cache:', HF_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108d5990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches (full): 2500 | Val batches: 2500\n",
      "Expected val steps per full validation: 2500\n"
     ]
    }
   ],
   "source": [
    "# 3. Dataloaders (validation split used as small corpus)\n",
    "from src.data.dataloader import make_coco_dataloader\n",
    "BATCH_SIZE = int(os.environ.get('DIAG_BS', 2))\n",
    "NUM_WORKERS = int(os.environ.get('DIAG_NUM_WORKERS', 2))\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "TRAIN_SPLIT = 'validation'\n",
    "VAL_SPLIT = 'validation'\n",
    "train_loader = make_coco_dataloader(split=TRAIN_SPLIT, batch_size=BATCH_SIZE, shuffle=True, caption_index=None, seed=SEED, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, cache_dir=HF_CACHE)\n",
    "val_loader   = make_coco_dataloader(split=VAL_SPLIT,   batch_size=BATCH_SIZE, shuffle=False, caption_index=None, seed=SEED, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, cache_dir=HF_CACHE)\n",
    "print('Train batches (full):', len(train_loader), '| Val batches:', len(val_loader))\n",
    "print('Expected val steps per full validation:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14623ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image count: 2\n",
      "First caption sample: A woman standing in front of a fruit stand.\n",
      "Image modes in batch: ['RGB', 'RGB']\n"
     ]
    }
   ],
   "source": [
    "# 4. Inspect First Batch\n",
    "first_images, first_caps = next(iter(train_loader))\n",
    "print('Batch image count:', len(first_images))\n",
    "print('First caption sample:', first_caps[0][:120])\n",
    "# Show channel modes if PIL Images\n",
    "modes = [getattr(img, 'mode', 'NA') for img in first_images]\n",
    "print('Image modes in batch:', modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31cceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale in first batch: 0/2\n"
     ]
    }
   ],
   "source": [
    "# 5. Channel / Grayscale Scan (quick)\n",
    "from collections import Counter\n",
    "def is_grayscale(img):\n",
    "    if hasattr(img, 'mode'):\n",
    "        return img.mode in ('L','1','I','F')\n",
    "    return False\n",
    "gs = sum(is_grayscale(im) for im in first_images)\n",
    "print(f'Grayscale in first batch: {gs}/{len(first_images)}')\n",
    "if gs: print('Consider converting to RGB inside dataloader transform.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ad4b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c80572dfdb54460bb287122d6e8a22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0.66M / Total: 297.45M\n"
     ]
    }
   ],
   "source": [
    "# 6. Build Model + Param Count\n",
    "from src.models.caption_modelling import GemmaDinoImageCaptioner\n",
    "from src.utils.training import LitCaptioner\n",
    "model = GemmaDinoImageCaptioner(include_cls=True, include_registers=False, include_patches=False, freeze_gemma=True)\n",
    "lit = LitCaptioner(model, optimizer_cfg={'lr':1e-4,'weight_decay':0.01,'betas':(0.9,0.999),'eps':1e-8})\n",
    "trainable = sum(p.numel() for p in lit.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in lit.parameters())\n",
    "print(f'Trainable params: {trainable/1e6:.2f}M / Total: {total/1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2279a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN/Inf in parameters at init. []\n"
     ]
    }
   ],
   "source": [
    "# 7. Parameter NaN / Inf Scan\n",
    "bad = []\n",
    "for n,p in lit.named_parameters():\n",
    "    if torch.isnan(p).any(): bad.append((n,'NaN'))\n",
    "    elif torch.isinf(p).any(): bad.append((n,'Inf'))\n",
    "print('Param anomalies:' if bad else 'No NaN/Inf in parameters at init.', bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50045b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward loss (fp32): 6.289526462554932\n",
      "OK: finite loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89809/247447131.py:7: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
      "  print('Forward loss (fp32):', float(loss))\n"
     ]
    }
   ],
   "source": [
    "# 8. Single Forward (FP32) Without Lightning\n",
    "model.eval()\n",
    "sample_images, sample_caps = next(iter(train_loader))\n",
    "with torch.autocast(device_type='cuda', enabled=False) if torch.cuda.is_available() else torch.no_grad():\n",
    "    out = model(sample_images, sample_caps)\n",
    "loss = out.loss if hasattr(out,'loss') else out['loss']\n",
    "print('Forward loss (fp32):', float(loss))\n",
    "assert torch.isfinite(loss), 'Loss is not finite (fp32 forward).'\n",
    "print('OK: finite loss.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56abba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All captions have at least one non-pad token.\n",
      "Tokenized shape: torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "# 9. Tokenization Sanity\n",
    "tok = model.gemma_tokenizer(sample_caps, return_tensors='pt', padding=True)\n",
    "ids = tok['input_ids']\n",
    "pad_id = model.gemma_tokenizer.pad_token_id\n",
    "empty_mask = (ids!=pad_id).sum(dim=1)==0\n",
    "if empty_mask.any():\n",
    "    print('Empty captions detected at indices:', torch.nonzero(empty_mask).view(-1).tolist())\n",
    "else:\n",
    "    print('All captions have at least one non-pad token.')\n",
    "print('Tokenized shape:', ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67273e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward loss (fp16 mixed): 6.268799304962158\n",
      "OK: finite mixed-precision loss.\n"
     ]
    }
   ],
   "source": [
    "# 10. Autocast (FP16) Forward Test\n",
    "if torch.cuda.is_available():\n",
    "    model.train()\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        out16 = model(sample_images, sample_caps)\n",
    "    loss16 = out16.loss if hasattr(out16,'loss') else out16['loss']\n",
    "    print('Forward loss (fp16 mixed):', float(loss16))\n",
    "    assert torch.isfinite(loss16), 'Loss is not finite under autocast.'\n",
    "    print('OK: finite mixed-precision loss.')\n",
    "else:\n",
    "    print('CUDA not available; skipping mixed precision test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7516372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89809/3115369058.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN grads (AMP).\n",
      "Gradient step loss: 7.310398578643799\n"
     ]
    }
   ],
   "source": [
    "# 11. Gradient Step (detect exploding grad / NaN)\n",
    "model.train()\n",
    "optim = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4, weight_decay=0.01)\n",
    "sample_images2, sample_caps2 = next(iter(train_loader))\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "def forward_loss(imgs, caps):\n",
    "    out = model(imgs, caps)\n",
    "    return out.loss if hasattr(out,'loss') else out['loss']\n",
    "optim.zero_grad()\n",
    "if use_amp:\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        l = forward_loss(sample_images2, sample_caps2)\n",
    "    scaler.scale(l).backward()\n",
    "    # Gradient anomaly detection\n",
    "    found_nan = False\n",
    "    for n,p in model.named_parameters():\n",
    "        if p.grad is not None and torch.isnan(p.grad).any():\n",
    "            print('NaN grad in', n); found_nan=True; break\n",
    "    if not found_nan: print('No NaN grads (AMP).')\n",
    "    scaler.step(optim); scaler.update()\n",
    "else:\n",
    "    l = forward_loss(sample_images2, sample_caps2)\n",
    "    l.backward()\n",
    "    for n,p in model.named_parameters():\n",
    "        if p.grad is not None and torch.isnan(p.grad).any():\n",
    "            print('NaN grad in', n); break\n",
    "    optim.step()\n",
    "print('Gradient step loss:', float(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f92fa842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcc939ce236404e9f1277f19f3a459e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d04d32d0d847e3911f15b9245750a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# 12. Mini Lightning Trainer (few steps, limit val)\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "mini_lit = LitCaptioner(model, optimizer_cfg={'lr':1e-4,'weight_decay':0.01,'betas':(0.9,0.999),'eps':1e-8})\n",
    "precision = '16-mixed' if torch.cuda.is_available() else '32-true'\n",
    "trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu', devices=1, max_steps=5, log_every_n_steps=1, precision=precision, limit_val_batches=2, enable_checkpointing=False, enable_model_summary=False, callbacks=[TQDMProgressBar(refresh_rate=1)])\n",
    "trainer.fit(mini_lit, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e50d297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train grayscale: 0/100 (0.0%)\n",
      "Val   grayscale: 0/100 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# 13. Full Dataset Channel Distribution Summary (sampled)\n",
    "import itertools\n",
    "def count_channels(dataloader, max_batches=50):\n",
    "    gray = 0; total = 0\n",
    "    for i,(imgs,caps) in enumerate(dataloader):\n",
    "        for im in imgs:\n",
    "            if getattr(im,'mode',None) in ('L','1','I','F'): gray+=1\n",
    "            total+=1\n",
    "        if i+1>=max_batches: break\n",
    "    return gray, total\n",
    "gray_train, tot_train = count_channels(train_loader)\n",
    "gray_val, tot_val = count_channels(val_loader)\n",
    "print(f'Train grayscale: {gray_train}/{tot_train} ({100*gray_train/max(1,tot_train):.1f}%)')\n",
    "print(f'Val   grayscale: {gray_val}/{tot_val} ({100*gray_val/max(1,tot_val):.1f}%)')\n",
    "if gray_train or gray_val:\n",
    "    print('NOTE: Convert grayscale -> RGB in dataloader to avoid channel mismatch issues.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff315bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostics complete. If no assertions failed, model/data pipeline is numerically stable for initial steps.\n",
      "Next: adjust LR, unfreeze more layers, or extend training steps as needed.\n"
     ]
    }
   ],
   "source": [
    "# 14. Summary & Next Steps\n",
    "print('Diagnostics complete. If no assertions failed, model/data pipeline is numerically stable for initial steps.')\n",
    "print('Next: adjust LR, unfreeze more layers, or extend training steps as needed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
