{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9956d78",
   "metadata": {},
   "source": [
    "# Sanity Train: Gemma + DINO on COCO (val)\n",
    "\n",
    "Run a quick 20-step training on GPU using the repo modules to verify end-to-end wiring.\n",
    "\n",
    "- Uses the COCO validation split (5k) for both train/val to keep it small.\n",
    "- Disables Weights & Biases logging.\n",
    "- Mixed precision on GPU if available.\n",
    "\n",
    "Tip: The first run downloads models/dataset (1â€“2GB). Subsequent runs will reuse cache (HF_HOME)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38311c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.3\n",
      "Torch: 2.8.0+cu128\n",
      "Transformers: 4.56.1\n",
      "datasets: 4.1.0\n",
      "Lightning: 2.5.5\n",
      "CUDA available -> NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# Environment & versions (local vars, no env mutation)\n",
    "import os, sys\n",
    "import torch\n",
    "import transformers\n",
    "import datasets as hfds\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "HF_CACHE = None\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Torch:', torch.__version__)\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('datasets:', hfds.__version__)\n",
    "print('Lightning:', pl.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available ->', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('CUDA not available; this run expects a GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make project root importable when running from scripts/\n",
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0876de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch -> images: 2, example caption: A bed with a pillow at the tip is neatly made. ...\n"
     ]
    }
   ],
   "source": [
    "# Build COCO validation loaders (used for both train and val)\n",
    "from src.data.dataloader import make_coco_dataloader\n",
    "\n",
    "batch_size = int(os.environ.get('SANITY_BS', 2))  # keep small for VRAM\n",
    "num_workers = int(os.environ.get('SANITY_NUM_WORKERS', 2))\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = make_coco_dataloader(\n",
    "    split='validation',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    caption_index=None,  # random choice for some variety\n",
    "    seed=42,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    cache_dir=HF_CACHE,\n",
    ")\n",
    "\n",
    "val_loader = make_coco_dataloader(\n",
    "    split='validation',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    caption_index=None,\n",
    "    seed=42,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    cache_dir=HF_CACHE,\n",
    ")\n",
    "\n",
    "sample_images, sample_caps = next(iter(train_loader))\n",
    "print(f'Sample batch -> images: {len(sample_images)}, example caption: {sample_caps[0][:80]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829134e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0de4edeb6d4518953be52c4d93598e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready. Trainable params: 657408\n"
     ]
    }
   ],
   "source": [
    "# Build model and LightningModule\n",
    "from src.models.caption_modelling import GemmaDinoImageCaptioner\n",
    "from src.utils.training import LitCaptioner\n",
    "\n",
    "# Use defaults from the module (Gemma-3 270M + DINOv3 Small).\n",
    "# Freeze Gemma for a quick adapter-only sanity run.\n",
    "model = GemmaDinoImageCaptioner(\n",
    "    include_cls=True, include_registers=False, include_patches=False,\n",
    "    freeze_gemma=True,\n",
    ")\n",
    "\n",
    "optimizer_cfg = {\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'eps': 1e-8,\n",
    "}\n",
    "\n",
    "lit = LitCaptioner(model, optimizer_cfg=optimizer_cfg)\n",
    "print('Model ready. Trainable params:', sum(p.numel() for p in lit.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4493953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/divyansh/Documents/SEM4/DL/Project/.venv/lib/python3.13/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                    | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | model | GemmaDinoImageCaptioner | 297 M  | train\n",
      "----------------------------------------------------------\n",
      "657 K     Trainable params\n",
      "296 M     Non-trainable params\n",
      "297 M     Total params\n",
      "1,189.794 Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "512       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57492c4a44fe4d0ca0f3c6e3a47f6928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/divyansh/Documents/SEM4/DL/Project/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 512 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd5962da71841ca953e030eebc407bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done. Checkpoints/logs -> /home/divyansh/Documents/SEM4/DL/Project/scripts/outputs/sanity_run\n"
     ]
    }
   ],
   "source": [
    "# Trainer: 20 optimizer steps on GPU if available\n",
    "assert torch.cuda.is_available(), 'This sanity check expects a GPU. Please enable CUDA.'\n",
    "precision = '16-mixed' if torch.cuda.is_available() else '32-true'\n",
    "out_dir = os.path.join(os.getcwd(), 'outputs', 'sanity_run')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_steps=20,\n",
    "    log_every_n_steps=1,\n",
    "    precision=precision,\n",
    "    limit_val_batches=5,  # keep validation quick\n",
    "    default_root_dir=out_dir,\n",
    ")\n",
    "\n",
    "trainer.fit(lit, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "print('Training done. Checkpoints/logs ->', out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3284d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] REF:  A child holding a flowered umbrella and petting a yak. END_PRED\n",
      "    PRED:  10000000000000000000000000000 END_PRED\n",
      "--------------------------------------------------------------------------------\n",
      "[2] REF:  A narrow kitchen filled with appliances and cooking utensils. END_PRED\n",
      "    PRED:  2019\n",
      "\n",
      "\\section{The Role of the Family in the Development of Children}\n",
      "\\section{Introduction}\n",
      "\\section{ END_PRED\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tiny generate sanity check on two images\n",
    "images, refs = next(iter(val_loader))\n",
    "preds = model.inference_generate(images[:2], max_new_tokens=30, temperature=0.0)\n",
    "for i, (ref, pred) in enumerate(zip(refs[:2], preds), 1):\n",
    "    print(f'[{i}] REF:  {ref} END_PRED')\n",
    "    print(f'    PRED: {pred} END_PRED')\n",
    "    print('-'*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
